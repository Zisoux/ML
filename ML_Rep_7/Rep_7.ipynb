{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba010f6",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ì„ì˜ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdaf2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 200  # ìƒíƒœë‹¹ ìƒ˜í”Œ ìˆ˜\n",
    "\n",
    "def generate_state(label, mean, std, n=n_samples):\n",
    "    x = np.random.normal(mean[0], std[0], n)\n",
    "    y = np.random.normal(mean[1], std[1], n)\n",
    "    z = np.random.normal(mean[2], std[2], n)\n",
    "    return pd.DataFrame({'x': x, 'y': y, 'z': z, 'label': label})\n",
    "\n",
    "# ê° ìƒíƒœë³„ ë°ì´í„° ìƒì„±\n",
    "df_rest = generate_state('ì •ì§€', [0, 0, 0], [0.1, 0.1, 0.1])\n",
    "df_walk = generate_state('ê±·ê¸°', [0.8, 0.8, 0.8], [0.5, 0.5, 0.5])\n",
    "df_run = generate_state('ë›°ê¸°', [2.0, 2.0, 2.0], [1.2, 1.2, 1.2])\n",
    "## df_stairs = generate_state('ê³„ë‹¨ì˜¤ë¥´ê¸°', [0.8, 0.8, 1.5], [0.5, 0.5, 0.7])\n",
    "## df_fall = generate_state('ë„˜ì–´ì§', [3.5, 3.5, 3.5], [2.5, 2.5, 2.5])\n",
    "\n",
    "# ì „ì²´ ë°ì´í„° í•©ì¹˜ê¸°\n",
    "df_all = pd.concat([df_rest, df_walk, df_run]).reset_index(drop=True)\n",
    "\n",
    "# CSVë¡œ ì €ì¥\n",
    "df_all.to_csv(\"simulated_sensor_data.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"âœ”ï¸ ë°ì´í„° ìƒì„± ì™„ë£Œ! 'simulated_sensor_data.csv' ì €ì¥ë¨.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f51da",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ìˆ˜ì§‘, ì „ì²˜ë¦¬ ëª¨ë¸ ì„ ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d902a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 1. CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# CSV íŒŒì¼ì—ëŠ” ë°˜ë“œì‹œ 'x', 'y', 'z', 'label' ì»¬ëŸ¼ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "csv_path = 'simulated_sensor_data.csv'  # â† ì—¬ê¸°ì— ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì…ë ¥\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2. ë°ì´í„° ì‹œê°í™”\n",
    "sns.pairplot(df, hue='label')\n",
    "plt.suptitle(\"ì„¼ì„œ ë°ì´í„° ìƒíƒœë³„ ë¶„í¬\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# 3. ì „ì²˜ë¦¬\n",
    "X = df[['x', 'y', 'z']]\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì •ì˜\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # í‰ê°€ ì§€í‘œ ì €ì¥\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, average='macro'),\n",
    "        'Recall': recall_score(y_test, y_pred, average='macro'),\n",
    "        'F1 Score': f1_score(y_test, y_pred, average='macro')\n",
    "    })\n",
    "\n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion Matrix ì‹œê°í™”\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6. ëª¨ë¸ ë¹„êµ ê²°ê³¼ ì¶œë ¥\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "print(results_df.sort_values(by='F1 Score', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84694c6f",
   "metadata": {},
   "source": [
    "# Streamlit ì•± ë°°í¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eda82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "st.set_page_config(layout=\"wide\")\n",
    "st.title(\"ğŸ‘· ê³ ì†Œ ì‘ì—…ì ìƒíƒœ ë¶„ë¥˜ ML ì‹œìŠ¤í…œ\")\n",
    "st.write(\"ì„¼ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì—¬ ì—¬ëŸ¬ ML ëª¨ë¸ì„ í‰ê°€í•˜ê³  ì˜ˆì¸¡í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# íŒŒì¼ ì—…ë¡œë“œ\n",
    "train_file = st.file_uploader(\"ğŸ“¥ í•™ìŠµìš© CSV ì—…ë¡œë“œ (x, y, z, label í¬í•¨)\", type=\"csv\")\n",
    "test_file = st.file_uploader(\"ğŸ“¥ ì˜ˆì¸¡ìš© CSV ì—…ë¡œë“œ (x, y, z)\", type=\"csv\")\n",
    "\n",
    "if train_file:\n",
    "    df = pd.read_csv(train_file)\n",
    "\n",
    "    # ë°ì´í„° í™•ì¸\n",
    "    if not {'x', 'y', 'z', 'label'}.issubset(df.columns):\n",
    "        st.error(\"âŒ í•™ìŠµ ë°ì´í„°ì—ëŠ” 'x', 'y', 'z', 'label' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        # ì „ì²˜ë¦¬\n",
    "        X = df[['x', 'y', 'z']]\n",
    "        y = df['label']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # ëª¨ë¸ ì •ì˜\n",
    "        models = {\n",
    "            'KNN': KNeighborsClassifier(),\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "            'Decision Tree': DecisionTreeClassifier(),\n",
    "            'Random Forest': RandomForestClassifier(),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(),\n",
    "            'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "        }\n",
    "\n",
    "        results = []\n",
    "        predictions_dict = {}\n",
    "\n",
    "        st.subheader(\"ğŸ“Š ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ê²°ê³¼\")\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            predictions_dict[name] = model  # ì €ì¥í•´ë‘ê¸°\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'Accuracy': acc,\n",
    "                'Precision': prec,\n",
    "                'Recall': rec,\n",
    "                'F1 Score': f1\n",
    "            })\n",
    "\n",
    "        results_df = pd.DataFrame(results).sort_values(by='F1 Score', ascending=False)\n",
    "        st.dataframe(results_df.style.highlight_max(axis=0))\n",
    "\n",
    "        # Confusion Matrix ì‹œê°í™”\n",
    "        st.subheader(\"ğŸ“‰ Confusion Matrix\")\n",
    "        selected_for_cm = st.selectbox(\"ğŸ” Confusion Matrix ë³´ê¸°: ëª¨ë¸ ì„ íƒ\", list(models.keys()))\n",
    "        model = models[selected_for_cm]\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=np.unique(y), yticklabels=np.unique(y), ax=ax)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(f\"{selected_for_cm} - Confusion Matrix\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        # ì˜ˆì¸¡ìš© ë°ì´í„° ì²˜ë¦¬\n",
    "        if test_file:\n",
    "            test_df = pd.read_csv(test_file)\n",
    "            if not {'x', 'y', 'z'}.issubset(test_df.columns):\n",
    "                st.error(\"âŒ ì˜ˆì¸¡ìš© CSVì—ëŠ” 'x', 'y', 'z' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "            else:\n",
    "                st.subheader(\"ğŸ” ì˜ˆì¸¡ ì‹¤í–‰\")\n",
    "                selected_model = st.selectbox(\"ğŸ“Œ ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ\", results_df['Model'].tolist())\n",
    "                best_model = predictions_dict[selected_model]\n",
    "                X_input = test_df[['x', 'y', 'z']]\n",
    "                X_scaled = scaler.transform(X_input)\n",
    "                pred_labels = best_model.predict(X_scaled)\n",
    "                test_df['ì˜ˆì¸¡ëœ ìƒíƒœ'] = pred_labels\n",
    "\n",
    "                st.success(\"âœ… ì˜ˆì¸¡ ì™„ë£Œ\")\n",
    "                st.dataframe(test_df)\n",
    "\n",
    "                st.write(\"ğŸ“ˆ ìƒíƒœ ë¶„í¬\")\n",
    "                st.bar_chart(test_df['ì˜ˆì¸¡ëœ ìƒíƒœ'].value_counts())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
