{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d902a87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 1. CSV 파일에서 데이터 불러오기\n",
    "# CSV 파일에는 반드시 'x', 'y', 'z', 'label' 컬럼이 포함되어야 합니다.\n",
    "csv_path = 'your_sensor_data.csv'  # ← 여기에 실제 파일 경로 입력\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2. 데이터 시각화\n",
    "sns.pairplot(df, hue='label')\n",
    "plt.suptitle(\"센서 데이터 상태별 분포\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# 3. 전처리\n",
    "X = df[['x', 'y', 'z']]\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. 모델 리스트 정의\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# 5. 모델 학습 및 평가\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # 평가 지표 저장\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, average='macro'),\n",
    "        'Recall': recall_score(y_test, y_pred, average='macro'),\n",
    "        'F1 Score': f1_score(y_test, y_pred, average='macro')\n",
    "    })\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion Matrix 시각화\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6. 모델 비교 결과 출력\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n📊 모델 성능 비교:\")\n",
    "print(results_df.sort_values(by='F1 Score', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eda82f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "st.set_page_config(layout=\"wide\")\n",
    "st.title(\"👷 고소 작업자 상태 분류 ML 시스템\")\n",
    "st.write(\"센서 데이터를 업로드하여 여러 ML 모델을 평가하고 예측합니다.\")\n",
    "\n",
    "# 파일 업로드\n",
    "train_file = st.file_uploader(\"📥 학습용 CSV 업로드 (x, y, z, label 포함)\", type=\"csv\")\n",
    "test_file = st.file_uploader(\"📥 예측용 CSV 업로드 (x, y, z)\", type=\"csv\")\n",
    "\n",
    "if train_file:\n",
    "    df = pd.read_csv(train_file)\n",
    "\n",
    "    # 데이터 확인\n",
    "    if not {'x', 'y', 'z', 'label'}.issubset(df.columns):\n",
    "        st.error(\"❌ 학습 데이터에는 'x', 'y', 'z', 'label' 컬럼이 필요합니다.\")\n",
    "    else:\n",
    "        # 전처리\n",
    "        X = df[['x', 'y', 'z']]\n",
    "        y = df['label']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # 모델 정의\n",
    "        models = {\n",
    "            'KNN': KNeighborsClassifier(),\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "            'Decision Tree': DecisionTreeClassifier(),\n",
    "            'Random Forest': RandomForestClassifier(),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(),\n",
    "            'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "        }\n",
    "\n",
    "        results = []\n",
    "        predictions_dict = {}\n",
    "\n",
    "        st.subheader(\"📊 모델 학습 및 평가 결과\")\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            predictions_dict[name] = model  # 저장해두기\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'Accuracy': acc,\n",
    "                'Precision': prec,\n",
    "                'Recall': rec,\n",
    "                'F1 Score': f1\n",
    "            })\n",
    "\n",
    "        results_df = pd.DataFrame(results).sort_values(by='F1 Score', ascending=False)\n",
    "        st.dataframe(results_df.style.highlight_max(axis=0))\n",
    "\n",
    "        # Confusion Matrix 시각화\n",
    "        st.subheader(\"📉 Confusion Matrix\")\n",
    "        selected_for_cm = st.selectbox(\"🔎 Confusion Matrix 보기: 모델 선택\", list(models.keys()))\n",
    "        model = models[selected_for_cm]\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=np.unique(y), yticklabels=np.unique(y), ax=ax)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(f\"{selected_for_cm} - Confusion Matrix\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        # 예측용 데이터 처리\n",
    "        if test_file:\n",
    "            test_df = pd.read_csv(test_file)\n",
    "            if not {'x', 'y', 'z'}.issubset(test_df.columns):\n",
    "                st.error(\"❌ 예측용 CSV에는 'x', 'y', 'z' 컬럼이 필요합니다.\")\n",
    "            else:\n",
    "                st.subheader(\"🔍 예측 실행\")\n",
    "                selected_model = st.selectbox(\"📌 사용할 모델 선택\", results_df['Model'].tolist())\n",
    "                best_model = predictions_dict[selected_model]\n",
    "                X_input = test_df[['x', 'y', 'z']]\n",
    "                X_scaled = scaler.transform(X_input)\n",
    "                pred_labels = best_model.predict(X_scaled)\n",
    "                test_df['예측된 상태'] = pred_labels\n",
    "\n",
    "                st.success(\"✅ 예측 완료\")\n",
    "                st.dataframe(test_df)\n",
    "\n",
    "                st.write(\"📈 상태 분포\")\n",
    "                st.bar_chart(test_df['예측된 상태'].value_counts())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
