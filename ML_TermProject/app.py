import os
import pickle
import streamlit as st
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import xgboost as xgb
import datetime
from googleapiclient.discovery import build
from google.auth.transport.requests import Request
from google_auth_oauthlib.flow import InstalledAppFlow
from imblearn.over_sampling import SMOTE  # SMOTEë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶ˆê· í˜• ì²˜ë¦¬
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# âœ… í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ìœ íŠœë¸Œ ì±„ë„ ì¶”ì²œ ë° ê´€ë¦¬", layout="wide")

# âœ… CSS ìŠ¤íƒ€ì¼
st.markdown("""
    <style>
    .main > div {
        display: flex;
        justify-content: center;
    }
    .block-container {
        max-width: 800px;
        padding-left: 3rem;
        padding-right: 3rem;
    }
    .bar-container {
        width: 100%;
        background-color: #f0f0f0;
        border-radius: 10px;
        overflow: hidden;
        height: 20px;
        margin-top: 4px;
        margin-bottom: 16px;
    }
    .bar-fill {
        height: 100%;
        background-color: #6699ff;
        text-align: right;
        padding-right: 8px;
        color: white;
        line-height: 20px;
        font-size: 12px;
    }
    </style>
""", unsafe_allow_html=True)

# âœ… ì œëª© ë° ì„¤ëª…
st.title("ğŸ¥ ìœ íŠœë¸Œ ì±„ë„ ì¶”ì²œê¸° & ê´€ë¦¬ ì„œë¹„ìŠ¤")
st.markdown("ìœ íŠœë¸Œ ì±„ë„ ì¶”ì²œê³¼ ê´€ë¦¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ê´€ì‹¬ ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³  êµ¬ë…í•œ ì±„ë„ì„ ê´€ë¦¬í•´ë³´ì„¸ìš”!")

# OAuth 2.0 ì„¤ì •
SCOPES = ['https://www.googleapis.com/auth/youtube.readonly']

# âœ… ì¸ì¦ í•¨ìˆ˜
def authenticate():
    """OAuth 2.0 ì¸ì¦ì„ í†µí•œ ì‚¬ìš©ì ì¸ì¦"""
    creds = None
    # í† í° íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ìê²© ì¦ëª…ì„ ë¡œë“œ
    if os.path.exists('token.pickle'):
        with open('token.pickle', 'rb') as token:
            creds = pickle.load(token)

    # ìê²© ì¦ëª…ì´ ì—†ê±°ë‚˜ ë§Œë£Œëœ ê²½ìš° ìƒˆë¡œ ì¸ì¦
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            try:
                flow = InstalledAppFlow.from_client_secrets_file(
                    'client_secrets.json', SCOPES)
                creds = flow.run_local_server(port=8080)  # í¬íŠ¸ ë³€ê²½
                print("Authentication successful!")
            except Exception as e:
                print("Error during authentication:", e)

        with open('token.pickle', 'wb') as token:
            pickle.dump(creds, token)

    return creds

# ì¸ì¦ í›„ YouTube API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
creds = authenticate()
youtube = build('youtube', 'v3', credentials=creds)

# âœ… ë°ì´í„° ë¡œë“œ (ìœ íŠœë¸Œ ì±„ë„ ì¶”ì²œìš©)
@st.cache_data
def load_data():
    df = pd.read_csv("youtube_channels_500+.csv")
    df = df.dropna(subset=["ì„¤ëª…"]).copy()
    df["ì„¤ëª…"] = df["ì„¤ëª…"].astype(str)
    return df

# âœ… ë²¡í„°í™” í•¨ìˆ˜ (ìœ íŠœë¸Œ ì±„ë„ ì¶”ì²œìš©)
@st.cache_data
def vectorize_descriptions(descriptions):
    vectorizer = TfidfVectorizer(stop_words="english")
    X = vectorizer.fit_transform(descriptions)
    return X, vectorizer

# âœ… ì¶”ì²œ í•¨ìˆ˜ (ìœ íŠœë¸Œ ì±„ë„ ì¶”ì²œìš©)
def hybrid_recommend(df, vectorizer, X, user_input, alpha=0.6, top_n=5):
    df = df.copy()
    user_vec = vectorizer.transform([user_input])
    cosine_sim = cosine_similarity(user_vec, X).flatten()

    df["label"] = df["ì„¤ëª…"].apply(lambda x: 1 if user_input.lower() in x.lower() else 0)
    if df["label"].sum() > 0:
        clf = LogisticRegression(max_iter=1000)
        clf.fit(X, df["label"])
        pred_proba = clf.predict_proba(X)[:, 1]
    else:
        pred_proba = [0] * len(df)

    df["ìœ ì‚¬ë„"] = cosine_sim
    df["ëª¨ë¸ì ìˆ˜"] = pred_proba
    df["ìµœì¢…ì ìˆ˜"] = alpha * df["ëª¨ë¸ì ìˆ˜"] + (1 - alpha) * df["ìœ ì‚¬ë„"]
    return df.sort_values(by="ìµœì¢…ì ìˆ˜", ascending=False).head(top_n)

# âœ… ì‚¬ìš©ìê°€ êµ¬ë…í•œ ì±„ë„ ë° ì‹œì²­ ì—¬ë¶€ í™•ì¸ í•¨ìˆ˜
def get_subscribed_channels():
    channels = []
    request = youtube.subscriptions().list(
        part="snippet",
        mine=True,
        maxResults=50
    )
    response = request.execute()
    for item in response["items"]:
        channel = item["snippet"]["title"]
        channel_id = item["snippet"]["resourceId"]["channelId"]
        channels.append((channel, channel_id))
    return channels

# âœ… ì‚¬ìš©ìê°€ ì‹œì²­í•œ ì˜ìƒ ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ë³´ì§€ ì•Šì€ ì±„ë„ì„ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
def get_unwatched_channels(days):
    unwatched_channels = set()  # ì§‘í•©ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì¤‘ë³µì„ ë°©ì§€
    last_viewed_date = datetime.datetime.now() - datetime.timedelta(days=days)
    
    # êµ¬ë…í•œ ì±„ë„ ëª©ë¡ì„ ë¶ˆëŸ¬ì™€ ë³´ì§€ ì•Šì€ ì±„ë„ì„ ì¶”ë ¤ëƒ…ë‹ˆë‹¤.
    channels = get_subscribed_channels()
    for channel, channel_id in channels:
        # í•´ë‹¹ ì±„ë„ì˜ ì˜ìƒì„ ê°€ì ¸ì˜¤ê³ , ì‚¬ìš©ìê°€ ë§ˆì§€ë§‰ìœ¼ë¡œ ë³¸ ì˜ìƒì„ í™•ì¸í•©ë‹ˆë‹¤.
        request = youtube.activities().list(
            part="snippet",
            channelId=channel_id,
            maxResults=5
        )
        response = request.execute()
        
        for activity in response["items"]:
            video_date = activity["snippet"]["publishedAt"]
            # fromisoformatì„ ì‚¬ìš©í•˜ì—¬ íƒ€ì„ì¡´ì„ í¬í•¨í•œ ë‚ ì§œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            video_date = datetime.datetime.fromisoformat(video_date)
            
            # íƒ€ì„ì¡´ ì •ë³´ ì œê±° (naive datetimeìœ¼ë¡œ ë³€í™˜)
            video_date = video_date.replace(tzinfo=None)
            
            if video_date < last_viewed_date:
                unwatched_channels.add(channel)  # ì§‘í•©ì— ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
    
    return list(unwatched_channels)  # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜


# âœ… ëª¨ë¸ í•™ìŠµ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
def train_and_evaluate_model(X_train, X_test, y_train, y_test):
    # ëª¨ë¸ë“¤ ì •ì˜
    models = {
        'Logistic Regression': LogisticRegression(max_iter=1000, C=1, solver='liblinear'),
        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10),
        'XGBoost': xgb.XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=6)
    }

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì • (ëœë¤ í¬ë ˆìŠ¤íŠ¸, XGBoost ì˜ˆì‹œ)
    param_grids = {
        'Logistic Regression': {
            'C': [0.1, 1, 10],
            'solver': ['liblinear', 'saga']
        },
        'Random Forest': {
            'n_estimators': [100, 200, 300],
            'max_depth': [5, 10, 15],
            'min_samples_split': [2, 5, 10]
        },
        'XGBoost': {
            'learning_rate': [0.01, 0.1, 0.3],
            'n_estimators': [100, 200],
            'max_depth': [3, 6, 9]
        }
    }

    best_models = {}
    best_scores = {}

    # StratifiedKFold ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ê°€ ë¶ˆê· í˜•í•  ë•Œë„ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë„ë¡ ì„¤ì •
    cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)  # cv=2ë¡œ ë³€ê²½

    # ì„±ëŠ¥ì„ ê¸°ë¡í•˜ê¸° ìœ„í•´ ì¶œë ¥ëœ ëª¨ë¸ ì„±ëŠ¥ì„ ì €ì¥
    st.subheader("ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    
    for model_name, model in models.items():
        st.write(f"Training {model_name}...")
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
        grid_search = GridSearchCV(model, param_grids[model_name], cv=cv, scoring='accuracy')  # StratifiedKFoldë¡œ êµì°¨ ê²€ì¦
        grid_search.fit(X_train, y_train)
        
        # ìµœì  ëª¨ë¸
        best_models[model_name] = grid_search.best_estimator_
        
        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = best_models[model_name].predict(X_test)
        
        # ì„±ëŠ¥ í‰ê°€
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        
        best_scores[model_name] = {
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1-Score': f1
        }
        
        st.write(f"**{model_name} ì„±ëŠ¥**:")
        st.write(f"Accuracy: {accuracy:.2f}")
        st.write(f"Precision: {precision:.2f}")
        st.write(f"Recall: {recall:.2f}")
        st.write(f"F1-Score: {f1:.2f}")
        st.markdown("---")
        
    return best_models, best_scores


# âœ… ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
col1, col2 = st.columns(2)

# ì™¼ìª½: ìœ íŠœë¸Œ ì±„ë„ ì¶”ì²œ ì„œë¹„ìŠ¤
with col1:
    st.subheader("ğŸ“Œ ìœ íŠœë¸Œ ì±„ë„ ì¶”ì²œ")
    user_input = st.text_input("ğŸ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì˜í™” ë¦¬ë·°, ê³ ì–‘ì´, ë¸Œì´ë¡œê·¸ ë“±):")
    alpha = st.slider("ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ê°€ì¤‘ì¹˜ (alpha)", 0.0, 1.0, 0.6, step=0.05)
    top_n = st.slider("ğŸ“Œ ì¶”ì²œ ì±„ë„ ê°œìˆ˜", 1, 10, 5)
    
    if user_input:
        df = load_data()
        X, vectorizer = vectorize_descriptions(df["ì„¤ëª…"])
        recommendations = hybrid_recommend(df, vectorizer, X, user_input, alpha=alpha, top_n=top_n)
        for _, row in recommendations.iterrows():
            score_percent = int(row["ìµœì¢…ì ìˆ˜"] * 100)
            st.markdown(f"### ğŸ”— [{row['ì±„ë„ëª…']}](https://www.youtube.com/channel/{row['ì±„ë„ID']})")
            st.markdown(f"- **ì¹´í…Œê³ ë¦¬:** {row['ì¹´í…Œê³ ë¦¬']}")
            st.markdown(f"- **êµ¬ë…ì ìˆ˜:** {row['êµ¬ë…ì ìˆ˜']}")
            st.markdown(f"- **ì˜ìƒ ìˆ˜:** {row['ì˜ìƒ ìˆ˜']}")

# ì˜¤ë¥¸ìª½: ì˜ ë³´ì§€ ì•ŠëŠ” ì±„ë„ ì •ë¦¬ ì„œë¹„ìŠ¤
with col2:
    st.subheader("ğŸ—‘ï¸ ì˜ ë³´ì§€ ì•ŠëŠ” ì±„ë„ ì •ë¦¬")
    days = st.slider("ì˜ ë³´ì§€ ì•Šì€ ì±„ë„ì„ íŒë‹¨í•  ê¸°ì¤€ ê¸°ê°„ì„ ì„ íƒí•˜ì„¸ìš” (ìµœì†Œ 3ì¼, ìµœëŒ€ 30ì¼):", 3, 30, 7)
    unwatched_channels = get_unwatched_channels(days)  # ì„ íƒí•œ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜ ë³´ì§€ ì•Šì€ ì±„ë„ ì¶”ì¶œ
    if unwatched_channels:
        st.markdown("**êµ¬ë… ì·¨ì†Œë¥¼ ì¶”ì²œí•˜ëŠ” ì±„ë„ë“¤**:") 
        for channel in unwatched_channels:
            st.markdown(f"- {channel}")
    else:
        st.markdown("**ëª¨ë“  ì±„ë„ì„ ì˜ ë³´ê³  ìˆìŠµë‹ˆë‹¤!**")

# ë°ì´í„° ì¤€ë¹„
df_channels = pd.DataFrame({
    'êµ¬ë…ì ìˆ˜': [1000, 1500, 2000, 800],
    'ì˜ìƒ ìˆ˜': [100, 50, 200, 10],
    'ë¼ë²¨': [1, 0, 1, 0]  # ì˜ˆì‹œ ë¼ë²¨: 1=ì˜ ë³´ê³  ìˆëŠ” ì±„ë„, 0=ì˜ ë³´ì§€ ì•Šì€ ì±„ë„
})

X = df_channels[['êµ¬ë…ì ìˆ˜', 'ì˜ìƒ ìˆ˜']]  # í”¼ì²˜
y = df_channels['ë¼ë²¨']  # ë¼ë²¨

# ë°ì´í„°ì…‹ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
best_models, best_scores = train_and_evaluate_model(X_train, X_test, y_train, y_test)

# ìµœì  ëª¨ë¸ ì„ íƒí•˜ì—¬ ì¶œë ¥
best_model_name = max(best_scores, key=lambda x: best_scores[x]['Accuracy'])  # ì •í™•ë„ê°€ ê°€ì¥ ë†’ì€ ëª¨ë¸ ì„ íƒ
best_model = best_models[best_model_name]

st.write(f"**ì„ íƒëœ ëª¨ë¸**: {best_model_name} - ì •í™•ë„: {best_scores[best_model_name]['Accuracy']:.2f}")
